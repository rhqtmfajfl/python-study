{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05. Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtVZKnW7zk8mXS42krkcM8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhqtmfajfl/python-study/blob/master/RNN/05_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucW2EaCcwwE6"
      },
      "source": [
        "# LSTM의 `return_sequences`, `return_state` 이해하기\n",
        "* `return_sequences` : 매 타임 스텝의 출력 여부 결정 결정\n",
        "* `return_state` : 제일 마지막 스테이트 출력 여부 결정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB8qbQOhxDPY"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import LSTM"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbM_AIvdxK9P"
      },
      "source": [
        "sample_train = np.random.randn(1, 4, 5) # N L I N 입력차원의 개 : (데이터 개수, 최대타임스탭, 임베딩차원)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6YVLLAHx8g9"
      },
      "source": [
        "return_sequences =False, return_state =False 확인\n",
        "* return _"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLJ-w_1YyN7L",
        "outputId": "6cca17c3-1827-4afd-fdfa-53d1a6ac842e"
      },
      "source": [
        "# 제일 마지막 HIDDEN STATE 만 반환 된다.\n",
        "\n",
        "last_hidden_state = LSTM(3, return_sequences=False, return_state=False)(sample_train)\n",
        "print(last_hidden_state) # 마지막 히든 스테이트만 나오게 된다."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.143588   0.18108416 0.10203055]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg2zkolpz31M"
      },
      "source": [
        "`return_sequences=False`, `return_state=True`\n",
        "\n",
        "* `hidden_states` = `last_hidden_state`\n",
        "* `last_cell_state`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEmUcfEQykDD",
        "outputId": "54a53135-485f-4c3e-9e96-0d026e35a87f"
      },
      "source": [
        "hidden_states, last_hidden_state, last_cell_state = LSTM(3, return_sequences=False, return_state=True)(sample_train)\n",
        "print(\"hidden_states : {}\".format(hidden_states))\n",
        "print(\"last_hidden_state : {}\".format(last_hidden_state))\n",
        "print(\"last_cell_state : {}\".format(last_cell_state))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_states : [[-0.10380044  0.08630534  0.30688265]]\n",
            "last_hidden_state : [[-0.10380044  0.08630534  0.30688265]]\n",
            "last_cell_state : [[-0.2548225   0.14245956  0.74640447]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua8oigiu1VCO"
      },
      "source": [
        "`return_sequences=True`, `return_state=False`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovf2P1PP1VqW",
        "outputId": "97f670b1-3ef3-4aff-a0de-f106df57178f"
      },
      "source": [
        "hidden_states = LSTM(3, return_sequences=True, return_state=False)(sample_train)\n",
        "print(\"hidden_states : {} / shape : {}\".format(hidden_states, hidden_states.shape))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_states : [[[-0.00298128  0.09771238 -0.01691602]\n",
            "  [-0.01515853  0.02933614 -0.05290803]\n",
            "  [ 0.0254513   0.10560504 -0.11466618]\n",
            "  [-0.01330078 -0.0996694   0.13423991]]] / shape : (1, 4, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9UwIBTw1WJZ"
      },
      "source": [
        "`return_sequences=True`,`return_state=True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3SvHReR1WMh",
        "outputId": "01d2b760-0896-4dc7-fb82-bed32b7fff2d"
      },
      "source": [
        "hidden_states, last_hidden_state, last_cell_state = LSTM(3, return_sequences=True, return_state=True)(sample_train)\n",
        "print(\"hidden_states : {}\".format(hidden_states))\n",
        "print(\"last_hidden_state : {}\".format(last_hidden_state))\n",
        "print(\"last_cell_state : {}\".format(last_cell_state))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_states : [[[-0.03499834  0.10745714 -0.10676076]\n",
            "  [ 0.00822138 -0.01179479 -0.16099004]\n",
            "  [ 0.34729257 -0.06188013  0.01959288]\n",
            "  [ 0.20290439  0.01924237 -0.01521631]]]\n",
            "last_hidden_state : [[ 0.20290439  0.01924237 -0.01521631]]\n",
            "last_cell_state : [[ 0.4077325   0.03281772 -0.02206334]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb3kEqUn1WPj"
      },
      "source": [
        "## Seq2Seq 챗봇 만들기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1pjgSUC1WTN",
        "outputId": "49246f5d-3f06-49a8-8490-088adf6c5e5a"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ZpYxWR1WWZ"
      },
      "source": [
        "import random # 나중에 데이터 셔플링 할 예정\n",
        "import tensorflow as tf\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgCxPapw1WZq"
      },
      "source": [
        "# 하이퍼 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFSR2VpU1Wd-"
      },
      "source": [
        "num_epochs = 200\n",
        "vocab_size = 2000"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hkHrPpp1Whi"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uD4DPv34A8C"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.emb = tf.keras.layers.Embedding(vocab_size, 64) # 임베딩차원도 64개\n",
        "    \n",
        "    # 제일 마지막 state를 리턴해야 context vector가 나옴!\n",
        "    self.lstm = tf.keras.layers.LSTM(512, return_sequences=False, return_state=True)\n",
        "\n",
        "\n",
        "  def call(self, training=False):\n",
        "    # 데이터가 들어온다 \n",
        "    # 임베딩\n",
        "    x = self.emb(x)\n",
        "\n",
        "    # Encoder에서는 context vector 만 얻어내면 되기 때문에 각 time 별 state는필요가 없다.\n",
        "    _, h, c = self.lstm(x)\n",
        "\n",
        "    #context vector return\n",
        "    return h, c"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QYp0i854DmD"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzxaN7Oj4Gur"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    # 여기도 임베딩이 있어야 하낟.\n",
        "    self.emb= tf.keras.layers.Embedding(vocab_size, 64) # 언어 인코딩 만들려면 각 언어들마다의 임베딩 벡터를 만들어야 한다.\n",
        "    self.lstm = tf.keras.layers.LSTM(512, return_sequences = True, return_state=True )\n",
        "    \n",
        "    #각 셀마다 2000개의 output을내고, 어떤 단어가추정 되었을 지를 계산\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax') # 단어 2000개의 데이터에서 softmax의 결과를 낸다.\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    #입력된  데이터들\n",
        "    x, h, c = inputs # shifted, (hidden_state, cell_state) #한칸 밀린데이터가 들어온다.\n",
        "    # 히든 스테이트고 셀스테이트기도 하지만 그냥 값이다.\n",
        "    x = self.emb(x) # 입력한 단어에 대한 임베딩벡터\n",
        "     # wx+b에 대한 값만 기억하고 있기\n",
        "\n",
        "\n",
        "    #y_ : 해당 시퀀스의 hidden_state\n",
        "    y_, h, c = self.lstm(x, initial_state=[h,c]) # initial_state : 초기화할 hidden_state, cell_state를 지정\n",
        "    \n",
        "    # y 값이랑 h, c 가 다음 단계로 넘어갈 값이 된다.\n",
        "\n",
        "    y = self.dense(y_)\n",
        "\n",
        "    return y, h, c"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTnRgvrA4KaG"
      },
      "source": [
        "# Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GabcvaF4M7t"
      },
      "source": [
        "class Seq2seq(tf.keras.Model):\n",
        "  #sos최초값을 넣느다.eos도 넣는다.\n",
        "  def __init__(self, sos, eos):\n",
        "    self.sos = sos # decoder에서 사용되어질 sos\n",
        "    self.eos = eos # encoder에서 사용되어질 eos\n",
        "    \n",
        "    self.enc = Encoder()\n",
        "    self.dec = Decoder() # enc와 dec를 엮어 줄 것이다.\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    if training: # 훈련시에서는 Teacher Forcing 때문에 정답이 들어옴\n",
        "      x, y  = inputs # 정답이 그대로들어감 기본 입력데이ㅓ(output_labels, shifted labels)\n",
        "\n",
        "      h, c = self.enc(x) # context vector가 등장 (들어감) 인코더에 대한 컨텍스트 벡터를 받아옴\n",
        "\n",
        "      # y의 shape은 전체 데이터를 모두 가지고 있는 상태가 된다.\n",
        "      # y.shape :(N, 65, 64) # 최대 단어수 64개가 64개씩 들어간다.\n",
        "      y, _, ___ = self.dec((y,h, c))#teacher forcing Decoder의 입력으로 Shifted Output을 넣어줌\n",
        "\n",
        "      #길이가 정해진 값들이 들어오게 됨\n",
        "      \n",
        "\n",
        "      return y # 훈련된 결과가 나옴\n",
        "    else: # 테스트 할 때는 x 만 들어온다.\n",
        "      x = inputs\n",
        "      # x엗 해나 히든과 셀스테이트\n",
        "      h, c = self.enc(x) # last_cell_state, last_hidden_state\n",
        "\n",
        "      # 시퀀셜 스테이트는없다.\n",
        "\n",
        "      #<sos> 입력\n",
        "      #1. <sos> 토큰을 tensor 배열화 시켜야함\n",
        "      y = tf.convert_to_tensor(self.sos) # 0 rank tensor로 변환\n",
        "      y = tf.reshape(y, (1,1)) # <sos> 가 (1,1) 형식으로 변환 (1배치, 1타임 스텝)을 의미, embedding 레이어에 넣을 예정\n",
        "\n",
        "      # 최대 입력 길이 만큼의 공간을미리 만들어 놓자\n",
        "      seq = tf.TensorArray(tf.int32, 64) # 64개의 텐서 배열 만들어 놓기\n",
        "    \n",
        "      # tensorflow의 session 환경에서 for 무을 조금더 빠르게 돌릴 수 있따.\n",
        "      for idx in tf.range(64):\n",
        "        # 제일 첨음엔 <sos>, 인코더의 h, c (context vector)가 들어간다.\n",
        "\n",
        "        ###########################################################################################################여기가 핵심\n",
        "        y, h, c = self.dec([y,h,c]) # 여기서 리턴 받는 y는 softmax의 결과 다음 시퀀스에 대한 소프트맥스 결과  \n",
        "        #######################################################################################################\n",
        "        #\n",
        "        \n",
        "        y = tf.cas(tf.argmax(y, axis= -1), dtype=tf.int32) #-1은 y는 0부터 1999중에 가장 큰것이 등장\n",
        "\n",
        "        # 한문장 들어가서 하나의 결과, 1배치 들어가니까 나오는 것도 1개\n",
        "        y = tf.reshape(y, (1,1)) # (1 배치, 1단어를 의미하기 위해 reshape - 테스트 할 때는 배치를 1로 설정할 예정) # 한문장\n",
        "\n",
        "        seq = seq.write(idx, y) # 순서대로 write\n",
        "\n",
        "        if y == self.eos:\n",
        "          break\n",
        "\n",
        "      return tf.reshape(seq.stack(), (1,64))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVUcYK4eMEQr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}