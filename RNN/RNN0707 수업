사람들이 어떤 단어를 찾을 경우 빈도수가 높으면 확률이 올라간다.

p 나는 버스를 탔다.
p1 나는 버스를 태운다.

p를 칠 확률이 p1을 칠 확률보다 높다.

문장을 만들때 확률 

기본적으로 첫번째는 `단어 시퀀스`(문장)를 이용해 더 그럴듯한 문장을 선택 - > 기계번역이라고도 한다.


기계번역(Machine Translation) 
p(나는 버스를 탔따) > p1(나는 버스를 태운다)
p가 더 확률이 높기 때문에 p를 만들어 낸다.

음성인식(Spell Correction)
발음이어떤가에 따라
p(나는 메롱을 먹는다) p1(나는 메론을 먹는다.)
p(이건 십이야) p1(이건시리야~)

오타교정(Spell Collection)
- 오타가 안난 단어를 예측 해준다.
-p(선생님이잘려갔다.) < p1(선생님이 달려갔다.)

어떤한 a사건이 일어날때 b라는 사건이 일어날 확률
-> 조건부 확률

언어모델에서의 확률 (두가지)

- 주어진 단어 시퀀스 다음에 어떤 단어가 등장할까? -> lm

1. 전체 단어 시퀀스의 확률
 - 어떤 단어를 쳤을 때 자동으로 단어를 완성시켜준다.
	예시 딥런ㅇ ㅣ -> 딥러닝이 나온다.

2. 이전 단어들이주어졌을 때다음단어의 등장 확률

- w1w2w3w4가 주어졌다.
p(w5)가 등장할 확률
전체 단어 시퀀스 p(w)다음 단어의 
전체 n개 n번째 등장할 확률을 구하는 것과 같다.

시작점의 확률ㅇㄹ 사용자가 입력을 한다.

전체 단어의 시퀀스는 n번째 등장할 단어의 확률

완성된 n개의 단어가 등장할 확률 (다음단어가등장할 확률)

딥러닝을쳤을 때
머신러닝과 딥러닝 차이가 가장 많이 나온다.

바이그램 단어 앞의 글자가 어떤 글자가 나왔는지 볼때


n 그램

n 번째를 예측 하기위해서 앞쪽 몇번째 단어 까지 볼것인가?



---------------------------------------------------------
NNLM(NeuralNetwork Language Model)

단어 임베딩이란 ->단어를 밀집 벡터로 만드는것(Dense Vector) = 유사한 단어를 찾기위함 
단어는 계층적 의미 구조를 가지고 있따.-
계층적 의미 궂 연속성과카테고리컬한 특성을 같이 가지고 있다.

단어가입으로 나오는순간 카테고리얼 된다.

에 핑크색을 말로 하면 하나의 카테고리가 된다.

하나의 핑크에도 핑크는여러가지가 있따. 핫핑크a, 핑크b, 핑크c 등 여러가지 핑크가 있따.


이러한 핑크 a,b,c,는 유사한 핑크이다.이런것을 벡터화 시킨다. (이것들 끼리를 유사한 벡터를 만들겠다. ->이것을 밀집 멕터라고 한다)

우리는 이미 밀집 베거를 만들어봤는데 그것은뉴런의 가중치이다.

뉴런마다 n개씩의 데이터를 받아서 
뉴런마다 가중치가 생긴다.  

실제 고양이를 구분하기 위한 벡터가 생기는데 고양이를 

잘 분류하기위한 값들이있따.

고양이는 하나의 특징을 유사하게 표현할 수 있게끔 표현 된다.

고양이는 고양이 대로 강아지는 강아지 대로 구분할 수 있따.

뉴런을 여러개 놓는 이유는 표현을 좀더 다양하게 함으로서 고양이의여러 특징들을 잡아내고강아지도 마찬가지로 여러 특징을잡아 낸다.


NNLM -> Shallow network (얕은신경망)




단어집합에서 내가 왕이 될 을 원핫 인코딩으로 만든다.

내가에 대한 단어 벡터





___________________________

내가왕이될상인가가올수 있도록 학습한다.

상인가 까지의 임베딩 테이블이 학습이 된다.

임베딩테이블이 학습되는 과정먼저 

훈련은각각의단어에대해 유사도를 측정한것

임베디드데이터


내가왕이될까지의유사도를구했을것이고,



-------------------------------------------------

t가 3이라면 yt는 어제 더워서를 반영한것이다.

